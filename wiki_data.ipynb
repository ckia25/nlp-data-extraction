{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Wiki Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was taken from https://github.com/noanabeshima/wikipedia-downloader\n",
    "# The dataset that is downloaded is the same as in the Pile: https://github.com/EleutherAI/the-pile?tab=readme-ov-file\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import fire\n",
    "\n",
    "def process_article(article):\n",
    "    # Converts an article to a single text file\n",
    "    title = article['title'].numpy().decode('UTF-8')\n",
    "    text = article['text'].numpy().decode('UTF-8')\n",
    "    return title+\"\\n\\n\"+text\n",
    "\n",
    "def main(n_jobs: int = 1):\n",
    "    # Downloads wikipedia dataset using tensorflow_datasets into 10 json files\n",
    "    try:\n",
    "        os.mkdir('output')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for interval in range(10):\n",
    "        if f'wikipedia-en-{interval}.json' not in os.listdir('./output'):\n",
    "            ds = tfds.load('wikipedia/20200301.en', split=f'train[{str(interval)}0%:{str(interval+1)}0%]')\n",
    "\n",
    "            result = Parallel(n_jobs=n_jobs)(delayed(process_article)(article) for article in tqdm(ds))\n",
    "\n",
    "            result = json.dumps(result)\n",
    "\n",
    "            file = open(f\"output/wikipedia-en-{interval}.json\", \"w\")\n",
    "            file.write(result)\n",
    "            file.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(main)\n",
    "\n",
    "# tensorflow==2.2.0\n",
    "# tfds-nightly==3.1.0.dev202007060105\n",
    "# fire==0.3.1\n",
    "# tqdm==4.47.0\n",
    "# joblib==0.15.1\n",
    "# apache-beam==2.22.0 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select random subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "def wiki_get_n_random_pages(n, file, num_chars): \n",
    "    with open(file, 'r') as raw_text:\n",
    "        json_file = json.loads(raw_text.read())\n",
    "        random_integers = np.random.choice(np.arange(0, len(json_file)), size=n, replace=False)\n",
    "        collection = [] \n",
    "        for i in random_integers:\n",
    "            subsection = json_file[i][0:min(len(json_file[i]), num_chars)]\n",
    "            title = subsection.split('\\n')[0]\n",
    "            text = subsection[len(title):]\n",
    "            collection.append([title, text])\n",
    "    return collection\n",
    "\n",
    "def collect_from_all_wiki(dir, n, num_chars, outfile):\n",
    "    dfs = []\n",
    "    count = 1\n",
    "    for filename in os.listdir(dir):\n",
    "        path = f\"{dir}/{filename}\"\n",
    "        collection = wiki_get_n_random_pages(n=n, file=path, num_chars=num_chars)\n",
    "        dfs.append(pd.DataFrame(collection, columns=[\"title\", \"text\"]))\n",
    "        print(f'finished file: {count}')\n",
    "        count += 1\n",
    "    df = pd.concat(dfs)\n",
    "    df.to_csv(outfile)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished file: 1\n",
      "finished file: 2\n",
      "finished file: 3\n",
      "finished file: 4\n",
      "finished file: 5\n",
      "finished file: 6\n",
      "finished file: 7\n",
      "finished file: 8\n",
      "finished file: 9\n",
      "finished file: 10\n"
     ]
    }
   ],
   "source": [
    "df = collect_from_all_wiki('output', 200, 2000, 'wiki_2000.csv').sample(frac=1).reindex()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiden1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
