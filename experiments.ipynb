{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaR25dV_Jcli"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlaC4iU_Iv55"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0yPAdxwgPgv6"
      },
      "outputs": [],
      "source": [
        "# model_param_string='14m'\n",
        "# model_param_string='70m'\n",
        "# model_param_string='160m'\n",
        "# model_param_string='410m'\n",
        "# model_param_string='1b'\n",
        "# model_param_string='1.4b'\n",
        "model_param_string='2.8b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsTR5j3dOLGU"
      },
      "outputs": [],
      "source": [
        "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
        "print(model_param_string)\n",
        "model = GPTNeoXForCausalLM.from_pretrained(\n",
        "  f\"EleutherAI/pythia-{model_param_string}-deduped\",\n",
        "  revision=\"step3000\",\n",
        "  cache_dir=f\"./pythia-{model_param_string}-deduped/step3000\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  f\"EleutherAI/pythia-{model_param_string}-deduped\",\n",
        "  revision=\"step3000\",\n",
        "  cache_dir=f\"./pythia-{model_param_string}-deduped/step3000\",\n",
        ")\n",
        "\n",
        "inputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\n",
        "tokens = model.generate(**inputs)\n",
        "tokenizer.decode(tokens[0])\n",
        "\n",
        "# REMOVE FOR CPU\n",
        "# model = model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x76FDUbLi_36"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jusf3Re_jNsr"
      },
      "source": [
        "#URL EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PhvWfwZTBd3a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def verify_url(url):\n",
        "  try:\n",
        "    if (requests.get(url=url, timeout=10).status_code) == 200:\n",
        "      print('Success')\n",
        "      return True\n",
        "  except Exception as ex:\n",
        "    print(ex)\n",
        "  return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LS-gqGoJBgvL"
      },
      "outputs": [],
      "source": [
        "def generate_urls(model, tokenizer, prefix='', num_sequences=1, max_length=20, min_length=0, k=50, num_gens=1):\n",
        "    seqs = []\n",
        "    inputs = tokenizer(prefix, return_tensors='pt')\n",
        "    tokens = model.generate(**inputs, max_length=max_length, min_length=min_length, do_sample=True, temperature=1, top_k=k, num_return_sequences=num_gens)\n",
        "    for row in tokens:\n",
        "      seqs.append([prefix, tokenizer.decode(row), False])\n",
        "    return seqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkPaECm9WH-9"
      },
      "outputs": [],
      "source": [
        "# URL EXPERIMENT DEPRECIATED\n",
        "seqs = generate_urls(model, tokenizer, 'https://', num_sequences=1, max_length=30, min_length=0, num_gens=200)\n",
        "for s in seqs:\n",
        "  url = s[1].split(' ')[0].split('\\n')[0]\n",
        "  print(url)\n",
        "  s[2] = verify_url(url)\n",
        "  s[1] = url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "CVgU4AJ8Ce5K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "count_url = 10\n",
        "df = pd.DataFrame(seqs, columns=['prefix', 'output', 'success'])\n",
        "df.to_csv(f'url_data_1_4b_{count_url}.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCfa7QBxjH91"
      },
      "source": [
        "#TOP-N Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a11YBNV9PCxG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_top_k(model, tokenizer, prefix='', num_sequences=1, max_length=20, min_length=0, k=50, num_gens=1):\n",
        "  seqs = []\n",
        "  rand=False\n",
        "  if prefix == '':\n",
        "    rand = True\n",
        "  for i in range(num_sequences):\n",
        "    if rand:\n",
        "      n = random.randint(3, model.config.vocab_size)\n",
        "      prefix = tokenizer.decode([n])\n",
        "    inputs = tokenizer(prefix, return_tensors='pt')\n",
        "    tokens = model.generate(**inputs, max_length=max_length, min_length=min_length, do_sample=True, top_k=k, num_return_sequences=num_gens)\n",
        "    for row in tokens:\n",
        "      seqs.append([prefix, tokenizer.decode(row)])\n",
        "  return seqs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OkECqPaWKYG"
      },
      "outputs": [],
      "source": [
        "# TOP-N Experiment\n",
        "count = 4\n",
        "seqs = generate_top_k(model, tokenizer, prefix=\"\", num_sequences=110, num_gens=10, max_length=100, min_length=100, k=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TK-Rshs78B"
      },
      "outputs": [],
      "source": [
        "print('prefix, output')\n",
        "for row in seqs:\n",
        "  print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8q-NXl3Wxcj"
      },
      "outputs": [],
      "source": [
        "# Save the results\n",
        "import pandas as pd\n",
        "i = 0\n",
        "for row in seqs:\n",
        "  if i % 10 == 0:\n",
        "    print(row)\n",
        "  i+=1\n",
        "df = pd.DataFrame(seqs, columns=['prefix', 'output'])\n",
        "df.to_csv(f'top_n_data{count}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42VOisrT7Eh5"
      },
      "outputs": [],
      "source": [
        "print(len(seqs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEQEPXUvXUHG"
      },
      "source": [
        "#Wiki Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oKqVgCHWRJt",
        "outputId": "6777c85e-fe31-4a06-bc0a-7a1aa9117efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[74, 717, 1469, 281, 253, 28974, 10873, 476, 368, 2619, 598, 253, 45160, 447, 4496]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer(\"i am going to the mall tomorrow can you pick up the groceries please\").input_ids)\n",
        "prefix = tokenizer.decode(tokenizer(\"i am going to the mall tomorrow can you pick up the groceries please\").input_ids[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "836iqd79Lqbx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def get_csv_data(csv_file):\n",
        "    return pd.read_csv(csv_file).to_numpy()[:,1:]\n",
        "\n",
        "def get_wiki_text(example_index, prefix_length, data, tokenizer):\n",
        "    prefix = tokenizer.decode(tokenizer(data[example_index][1][2:]).input_ids[:prefix_length])\n",
        "    text = data[example_index][1]\n",
        "    return text, prefix\n",
        "\n",
        "def wiki_generation(model, tokenizer, prefix, example_index, training_text, max_length=100, k=50, min_length=0, do_sample=True):\n",
        "    seqs = []\n",
        "    inputs = tokenizer(prefix, return_tensors='pt')\n",
        "    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
        "    tokens = model.generate(**inputs, max_length=max_length, min_length=min_length, do_sample=do_sample, temperature=1, top_k=k)\n",
        "    for row in tokens:\n",
        "      seqs.append([example_index, prefix, tokenizer.decode(row), training_text[2:]])\n",
        "    return seqs\n",
        "\n",
        "def convert_to_csv(rows, outfile):\n",
        "    df = pd.DataFrame(rows, columns=['Example Index', 'Prefix', 'Output', 'Training Text'])\n",
        "    df.to_csv(outfile)\n",
        "    return df\n",
        "\n",
        "def get_many_wiki_generations(model, tokenizer, start_index, prefix_token_length, max_token_generation_length, do_sample, outfile, end=None):\n",
        "    data = get_csv_data('shuffled_wiki_2000.csv')\n",
        "    rows = []\n",
        "    if end is None:\n",
        "        end = data.shape[0]\n",
        "    count = 0\n",
        "    for i in range(start_index, end):\n",
        "        text, prefix = get_wiki_text(i, prefix_token_length, data, tokenizer)\n",
        "        seqs = wiki_generation(model, tokenizer, prefix, i, text, max_length=max_token_generation_length, do_sample=do_sample)\n",
        "        for s in seqs:\n",
        "          rows.append(s)\n",
        "        count += 1\n",
        "        if count % 50 == 0:\n",
        "          print(f'Completed: {count}/{end - start_index}')\n",
        "    return convert_to_csv(rows, outfile)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCl1UhtkNTdx"
      },
      "outputs": [],
      "source": [
        "df = get_many_wiki_generations(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    start_index=0,\n",
        "    end=505,\n",
        "    prefix_token_length=20,\n",
        "    max_token_generation_length=100,\n",
        "    do_sample=False,\n",
        "    outfile=\"wiki_sample_2_8b_20_0_to_505.csv\"\n",
        ")\n",
        "# 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ib-3xpbwfnQY"
      },
      "outputs": [],
      "source": [
        "def check_for_memorization(prefix, generated, ref_text, tokenizer):\n",
        "    begin = ref_text.find(prefix)\n",
        "    # ref_text = ref_text[:]\n",
        "    # period = ref_text[:len(prefix) + 1].rfind('.')\n",
        "    # if period > -1:\n",
        "    #   ref_text = ref_text[:period+1] + ' ' + ref_text[period+2:]\n",
        "    #   # print(ref_text)\n",
        "\n",
        "\n",
        "    prefix_ids = tokenizer(prefix).input_ids\n",
        "    generated_ids = tokenizer(generated).input_ids\n",
        "    ref_ids = tokenizer(ref_text).input_ids\n",
        "\n",
        "    print('__________________________')\n",
        "    print(generated_ids[20:30])\n",
        "    print(ref_ids[20:30])\n",
        "\n",
        "\n",
        "    start_index = len(prefix_ids)\n",
        "    # if ref_ids[start_index - 1] != prefix_ids[-1]:\n",
        "    #   for k in range(3):\n",
        "    #     if ref_ids[start_index + k] == prefix_ids[-1]:\n",
        "    #       start_index += k + 1\n",
        "    #       for j in range(k + 1):\n",
        "    #         generated_ids.insert(0, 0)\n",
        "    #       break\n",
        "\n",
        "    # for i in reversed(range(len(prefix_ids))):\n",
        "    #   if ref_ids[i] == prefix_ids[i]:\n",
        "    #     start_index = i + 1\n",
        "    #     break\n",
        "\n",
        "\n",
        "    end = min(len(generated_ids), len(ref_ids))\n",
        "    count = 0\n",
        "    for i in range(start_index, end):\n",
        "        if ref_ids[i] == generated_ids[i]:\n",
        "            count+=1\n",
        "        else:\n",
        "          end = i\n",
        "          break\n",
        "    return count, tokenizer.decode(ref_ids[:end]), tokenizer.decode(generated_ids[:end])\n",
        "\n",
        "\n",
        "def check_all_samples(csv_file, tokenizer, prefix_token_length, model_name):\n",
        "    data = pd.read_csv(csv_file, index_col=0).to_numpy()\n",
        "    df_data = []\n",
        "    for row in data:\n",
        "        count, gen, ref = check_for_memorization(row[1], row[2], row[3], tokenizer)\n",
        "        df_data.append([row[0], row[1], row[2], row[3], count, gen, ref, prefix_token_length, model_name])\n",
        "    return pd.DataFrame(df_data, columns=['Example Id', 'Prefix', 'Output', 'Training Text', 'Matching Token Count', 'Matching Output', 'Matching Reference', 'Prefix Length', 'model'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJyX7WF6U1dH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = check_all_samples('wiki_sample_2_8b_20_0_to_505.csv', tokenizer, 20, '2.8b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np8va0VsNoYM"
      },
      "outputs": [],
      "source": [
        "display(df.sort_values('Example Id'))\n",
        "data = df.sort_values('Example Id')\n",
        "# for val in data[503][1:4]:\n",
        "#   print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p-WlqrbMlPR1"
      },
      "outputs": [],
      "source": [
        "data.to_csv('wiki_2_8b_20.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
